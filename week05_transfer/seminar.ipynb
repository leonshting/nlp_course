{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zriTdjauH8iQ"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQiRPWWHlSgv"
   },
   "source": [
    "### Using pre-trained transformers\n",
    "_for fun and profit_\n",
    "\n",
    "There are many toolkits that let you access pre-trained transformer models, but the most powerful and convenient by far is [`huggingface/transformers`](https://github.com/huggingface/transformers). In this week's practice, you'll learn how to download, apply and modify pre-trained transformers for a range of tasks. Buckle up, we're going in!\n",
    "\n",
    "\n",
    "__Pipelines:__ if all you want is to apply a pre-trained model, you can do that in one line of code using pipeline. Huggingface/transformers has a selection of pre-configured pipelines for masked language modelling, sentiment classification, question aswering, etc. ([see full list here](https://huggingface.co/transformers/main_classes/pipelines.html))\n",
    "\n",
    "A typical pipeline includes:\n",
    "* pre-processing, e.g. tokenization, subword segmentation\n",
    "* a backbone model, e.g. bert finetuned for classification\n",
    "* output post-processing\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rP1KFtvLlJHR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998860359191895}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "print(classifier(\"BERT is amazing!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nYUNuyXMn5l9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonsht/Programming/study/nlp_course/venv/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: decodestring() is a deprecated alias since Python 3.1, use decodebytes()\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "data = {\n",
    "    'arryn': 'As High as Honor.',\n",
    "    'baratheon': 'Ours is the fury.',\n",
    "    'stark': 'Winter is coming.',\n",
    "    'tyrell': 'Growing strong.'\n",
    "}\n",
    "\n",
    "# YOUR CODE: predict sentiment for each noble house and create outputs dict\n",
    "outputs = {k: classifier(v)[0]['label'] == 'POSITIVE' for k, v in data.items()}\n",
    "\n",
    "assert sum(outputs.values()) == 3 and outputs[base64.decodestring(b'YmFyYXRoZW9u\\n').decode()] == False\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRDhIH-XpSNo"
   },
   "source": [
    "You can also access vanilla Masked Language Model that was trained to predict masked words. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pa-8noIllRbZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P=0.99719 donald trump is the president of the united states.\n",
      "P=0.00024 donald duck is the president of the united states.\n",
      "P=0.00022 donald ross is the president of the united states.\n",
      "P=0.00020 donald johnson is the president of the united states.\n",
      "P=0.00018 donald wilson is the president of the united states.\n"
     ]
    }
   ],
   "source": [
    "mlm_model = pipeline('fill-mask', model=\"bert-base-uncased\")\n",
    "MASK = mlm_model.tokenizer.mask_token\n",
    "\n",
    "for hypo in mlm_model(f\"Donald {MASK} is the president of the united states.\"):\n",
    "    print(f\"P={hypo['score']:.5f}\", hypo['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9NxeG1Y5pwX1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'russian red revolution happened in the year 1917.',\n",
       "  'score': 0.5716407299041748,\n",
       "  'token': 4585,\n",
       "  'token_str': '1917'},\n",
       " {'sequence': 'russian red revolution happened in the year 1918.',\n",
       "  'score': 0.1523766964673996,\n",
       "  'token': 4271,\n",
       "  'token_str': '1918'},\n",
       " {'sequence': 'russian red revolution happened in the year 1919.',\n",
       "  'score': 0.021794825792312622,\n",
       "  'token': 4529,\n",
       "  'token_str': '1919'},\n",
       " {'sequence': 'russian red revolution happened in the year 1920.',\n",
       "  'score': 0.02099030464887619,\n",
       "  'token': 4444,\n",
       "  'token_str': '1920'},\n",
       " {'sequence': 'russian red revolution happened in the year 1916.',\n",
       "  'score': 0.02016862854361534,\n",
       "  'token': 4947,\n",
       "  'token_str': '1916'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your turn: use bert to recall what year was the Soviet Union founded in\n",
    "mlm_model(f\"Russian Red Revolution happened in the year {MASK}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'russian empire revolution happened in the year 1917.',\n",
       "  'score': 0.2406449317932129,\n",
       "  'token': 4585,\n",
       "  'token_str': '1917'},\n",
       " {'sequence': 'russian empire revolution happened in the year 1918.',\n",
       "  'score': 0.0716315284371376,\n",
       "  'token': 4271,\n",
       "  'token_str': '1918'},\n",
       " {'sequence': 'russian empire revolution happened in the year 1905.',\n",
       "  'score': 0.04560703784227371,\n",
       "  'token': 5497,\n",
       "  'token_str': '1905'},\n",
       " {'sequence': 'russian empire revolution happened in the year 1916.',\n",
       "  'score': 0.02266937680542469,\n",
       "  'token': 4947,\n",
       "  'token_str': '1916'},\n",
       " {'sequence': 'russian empire revolution happened in the year 1848.',\n",
       "  'score': 0.021684745326638222,\n",
       "  'token': 7993,\n",
       "  'token_str': '1848'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check if bert knows that there wass @give us some democracy@ revolution earlier this century\n",
    "mlm_model(f\"Russian Empire Revolution happened in the year {MASK}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hehe, it does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJxRFzCSq903"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Huggingface offers hundreds of pre-trained models that specialize on different tasks. You can quickly find the model you need using [this list](https://huggingface.co/models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HRux8Qp2hkXr"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Almost two-thirds of the 1.5 million people who viewed this liveblog had Googled to discover\n",
    " the latest on the Rosetta mission. They were treated to this detailed account by the Guardian’s science editor,\n",
    " Ian Sample, and astronomy writer Stuart Clark of the moment scientists landed a robotic spacecraft on a comet \n",
    " for the first time in history, and the delirious reaction it provoked at their headquarters in Germany.\n",
    "  “We are there. We are sittingon the surface. Philae is talking to us,” said one scientist.\n",
    "\"\"\"\n",
    "\n",
    "#from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Task: create a pipeline for named entity recognition, use task name 'ner' and search for the right model in the list\n",
    "ner_model = pipeline('ner', 'dbmdz/bert-large-cased-finetuned-conll03-english')\n",
    "named_entities = ner_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hf57MRzSiSON"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: [{'entity': 'I-MISC', 'score': 0.8746414, 'index': 19, 'word': 'Google', 'start': 73, 'end': 79}, {'entity': 'I-MISC', 'score': 0.8997781, 'index': 27, 'word': 'Rose', 'start': 112, 'end': 116}, {'entity': 'I-MISC', 'score': 0.951263, 'index': 28, 'word': '##tta', 'start': 116, 'end': 119}, {'entity': 'I-ORG', 'score': 0.99925184, 'index': 40, 'word': 'Guardian', 'start': 179, 'end': 187}, {'entity': 'I-PER', 'score': 0.9992004, 'index': 46, 'word': 'Ian', 'start': 207, 'end': 210}, {'entity': 'I-PER', 'score': 0.9995033, 'index': 47, 'word': 'Sam', 'start': 211, 'end': 214}, {'entity': 'I-PER', 'score': 0.9965073, 'index': 48, 'word': '##ple', 'start': 214, 'end': 217}, {'entity': 'I-PER', 'score': 0.9991747, 'index': 53, 'word': 'Stuart', 'start': 240, 'end': 246}, {'entity': 'I-PER', 'score': 0.9996471, 'index': 54, 'word': 'Clark', 'start': 247, 'end': 252}, {'entity': 'I-LOC', 'score': 0.9998199, 'index': 85, 'word': 'Germany', 'start': 414, 'end': 421}, {'entity': 'I-PER', 'score': 0.58480644, 'index': 99, 'word': 'Phil', 'start': 470, 'end': 474}, {'entity': 'I-PER', 'score': 0.8126388, 'index': 100, 'word': '##ae', 'start': 474, 'end': 476}]\n",
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "print('OUTPUT:', named_entities)\n",
    "word_to_entity = {item['word']: item['entity'] for item in named_entities}\n",
    "assert 'org' in word_to_entity.get('Guardian').lower() and 'per' in word_to_entity.get('Stuart').lower()\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULMownz6sP9n"
   },
   "source": [
    "### The building blocks of a pipeline\n",
    "\n",
    "Huggingface also allows you to access its pipelines on a lower level. There are two main abstractions for you:\n",
    "* `Tokenizer` - converts from strings to token ids and back\n",
    "* `Model` - a pytorch `nn.Module` with pre-trained weights\n",
    "\n",
    "You can use such models as part of your regular pytorch code: insert is as a layer in your model, apply it to a batch of data, backpropagate, optimize, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KMJbV0QVsO0Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZgSPHKPRxG6U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[ 101, 5355, 1010, 1045, 2572, 2115, 2269, 1012,  102,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101, 2166, 2003, 2054, 6433, 2043, 2017, 1005, 2128, 5697, 2437, 2060,\n",
      "         3488, 1012,  102]])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Detokenized:\n",
      "[CLS] luke, i am your father. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] life is what happens when you're busy making other plans. [SEP]\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    \"Luke, I am your father.\",\n",
    "    \"Life is what happens when you're busy making other plans.\",\n",
    "    ]\n",
    "\n",
    "# tokenize a batch of inputs. \"pt\" means [p]y[t]orch tensors\n",
    "tokens_info = tokenizer(lines, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "for key in tokens_info:\n",
    "    print(key, tokens_info[key])\n",
    "\n",
    "print(\"Detokenized:\")\n",
    "for i in range(2):\n",
    "    print(tokenizer.decode(tokens_info['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MJkbHxERyfL4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8854, -0.4722, -0.9392,  ..., -0.8081, -0.6955,  0.8748],\n",
      "        [-0.9297, -0.5161, -0.9334,  ..., -0.9017, -0.7492,  0.9201]])\n"
     ]
    }
   ],
   "source": [
    "# You can now apply the model to get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens_info)\n",
    "\n",
    "print(outputs['pooler_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMpzCoIL0Mmw"
   },
   "source": [
    "### The search for similar questions.\n",
    "\n",
    "Remeber week01 where you used GloVe embeddings to find related questions? That was.. cute, but far from state of the art. It's time to **really** solve this task using context-aware embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziLqtAcD3CXX"
   },
   "outputs": [],
   "source": [
    "# download the data:\n",
    "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt\n",
    "# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmKr7Eza3PZ6"
   },
   "source": [
    "__Main task(3 pts):__ \n",
    "* Implement a function that takes a text string and finds top-k most similar questions from `quora.txt`\n",
    "* Demonstrate your function using at least 5 examples\n",
    "\n",
    "There are no prompts this time: you will have to write everything from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What TV shows or books help you read people's body language?\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def cos_sim(u,v):\n",
    "    return 1 - cosine(u,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize a batch of inputs. \"pt\" means [p]y[t]orch tensors\n",
    "tokens_info = tokenizer(data[:10], padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detokenized:\n",
      "[CLS] can i get back with my ex even though she is pregnant with another guy's baby? [SEP]\n",
      "[CLS] what are some ways to overcome a fast food addiction? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(\"Detokenized:\")\n",
    "for i in range(2):\n",
    "    print(tokenizer.decode(tokens_info['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(lines, model, tokenizer, batch_size=512, feature_dim=768, device='cuda') -> np.ndarray:\n",
    "    nr_lines = len(lines)\n",
    "    \n",
    "    feature_matrix = np.zeros((nr_lines, feature_dim))\n",
    "    \n",
    "    batch_borders = list(range(0, nr_lines + batch_size, batch_size))\n",
    "    for batch_b, batch_e in tqdm.tqdm(zip(batch_borders[:-1], batch_borders[1:])):\n",
    "        batch_e = min(batch_e, nr_lines)\n",
    "\n",
    "        tokens_info = tokenizer(\n",
    "            lines[batch_b: batch_e], padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        tokens_info = {k: v.to(device) for k, v in tokens_info.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens_info)\n",
    "            feature_matrix[batch_b: batch_e] = outputs['pooler_output'].cpu().numpy()\n",
    "    \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1050it [10:34,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "features = get_features(data, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(features):\n",
    "    return features / np.linalg.norm(features, ord=2, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_l2 = normalize_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar(line, features_matrix, ref_lines, device='cuda', topn=5):\n",
    "    tokens_info = tokenizer(\n",
    "        [line], padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    tokens_info = {k: v.to(device) for k, v in tokens_info.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens_info)\n",
    "        feature = outputs['pooler_output'].cpu().numpy()\n",
    "        \n",
    "    similarities = 1 - features_matrix @ normalize_features(feature)[0];\n",
    "    indices = np.argsort(similarities)\n",
    "    return [(ref_lines[i], similarities[i]) for i in indices[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is like to live in Morocco as foreigner?\\n', 0.0029890035040398555),\n",
       " ('What is to be a Hindu in Pakistan?\\n', 0.0035213410771406384),\n",
       " ('How long does it take to renew Indian passport in Canada?\\n',\n",
       "  0.0035492363370205338),\n",
       " ('What is it like to live and work in London 20s?\\n', 0.00359940182220575),\n",
       " ('What is the best app for white men who only like black men?\\n',\n",
       "  0.0036031545994793523)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar('What is like to be white male in USA?', features_l2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is the difference between the Manchu and Han people?\\n',\n",
       "  0.002449484354391518),\n",
       " ('What is life like as a black person in China?\\n', 0.0024514532470841788),\n",
       " ('What do non-Indonesians think about rendang?\\n', 0.00254805326030505),\n",
       " ('What is it like to be an ethnic Tajik in China?\\n', 0.002566751405706613),\n",
       " ('What do Kurds think of Germans?\\n', 0.0027248765311250756)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar('What is like to be white male in Russia?', features_l2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Are there any real parallel universe?\\n', 0.002507165318368032),\n",
       " ('Are there any mathematical proofs to the existence of aliens?\\n',\n",
       "  0.00251653791223172),\n",
       " ('Is there a way to get infinite energy?\\n', 0.0028554901967061674),\n",
       " ('Are there really interpretations of dreams?\\n', 0.0028775753667864556),\n",
       " ('Is it true that continental drift is fake?\\n', 0.002924550808185211)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar('Are there parallel universes?', features_l2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is matrix learning?\\n', 0.0007113249482321171),\n",
       " ('What is backend process?\\n', 0.0008258957443837422),\n",
       " ('What is Fourier transform?\\n', 0.0008880005608729036),\n",
       " ('What is virtual machine?\\n', 0.0009063134571185572),\n",
       " ('What is neuro linguistic programming?\\n', 0.0009082031753746556)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar('What is neural network?', features_l2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('JavaScript vs Java?\\n', 0.0023425796013568645),\n",
       " ('Scala Vs Node.js?\\n', 0.002656501804128686),\n",
       " ('Ntpc gate cutoff?\\n', 0.0028276098248721793),\n",
       " ('Windows 10 architecture?\\n', 0.0032204057895649507),\n",
       " ('C++ free certificate?\\n', 0.0034634240529997085)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar('ARM vs x86?', features_l2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How do I improve solution for Josephus problem algorithm?\\n',\n",
       "  0.0034718122028002396),\n",
       " ('What is inverse function? What are some examples?\\n', 0.003655330969229542),\n",
       " ('What is tangential component of electric field?\\n', 0.003663591549844858),\n",
       " ('Is any new idea for usefull mini project in compuer engineering?\\n',\n",
       "  0.003782691745504607),\n",
       " ('How do I prove ratio test and root test for convergence-divergence using epsilon delta definition?\\n',\n",
       "  0.003845545280049989)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar('Can I do Discrete Fourier Transform in Galois field?', features_l2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How was Bank of America founded?\\n', 0.0020955360146868163),\n",
       " ('When and how was \"The Laboratory\" written?\\n', 0.0021501556443611625),\n",
       " (\"What was the main focus of Nazi Germany's SS? Why?\\n\",\n",
       "  0.0021622765529916155),\n",
       " ('How did the Hidden Wiki come to be?\\n', 0.0022083650866796534),\n",
       " ('What did CCCP mean in Russia?\\n', 0.0022590918248388547)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar('How Enigma was decrypted?', features_l2, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHEC6o7uAfgQ"
   },
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "__Bonus demo:__ transformer language models. \n",
    "\n",
    "`/* No points awarded for this task, but its really cool, we promise :) */`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWCajBGcAern"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').train(False).to(device)\n",
    "\n",
    "text = \"The Fermi paradox \"\n",
    "tokens = tokenizer.encode(text)\n",
    "num_steps = 1024\n",
    "line_length, max_length = 0, 70\n",
    "\n",
    "print(end=tokenizer.decode(tokens))\n",
    "\n",
    "for i in range(num_steps):\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.as_tensor([tokens], device=device))[0]\n",
    "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
    "\n",
    "    next_token_index = p_next.argmax() #<YOUR CODE: REPLACE THIS LINE>\n",
    "    # YOUR TASK: change the code so that it performs nucleus sampling\n",
    "\n",
    "    tokens.append(int(next_token_index))\n",
    "    print(end=tokenizer.decode(tokens[-1]))\n",
    "    line_length += len(tokenizer.decode(tokens[-1]))\n",
    "    if line_length >= max_length:\n",
    "        line_length = 0\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Vij7Gc1wOaq"
   },
   "source": [
    "Transformers knowledge hub: https://huggingface.co/transformers/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp-venv-37",
   "language": "python",
   "name": "nlp-venv-37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
